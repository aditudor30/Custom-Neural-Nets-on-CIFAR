{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b797fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022f73a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca526d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "471e9345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:20<00:00, 8.16MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.CIFAR100('./data', train = True, transform=train_transform, download = True)\n",
    "trainloader = DataLoader(trainset,batch_size = 64, shuffle = True, num_workers = 2)\n",
    "\n",
    "testset = datasets.CIFAR100('./data', train = False, transform = test_transform, download=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle = False, num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00208cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride = 2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 100)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc6f397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vgg_model(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Linear(in_features=1024, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vgg_model().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6fdf970",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size=20, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4aedab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Loss: 4.1387\n",
      "Epoch 2/50 | Loss: 3.6721\n",
      "Epoch 3/50 | Loss: 3.4267\n",
      "Epoch 4/50 | Loss: 3.2261\n",
      "Epoch 5/50 | Loss: 3.0317\n",
      "Epoch 6/50 | Loss: 2.8747\n",
      "Epoch 7/50 | Loss: 2.7322\n",
      "Epoch 8/50 | Loss: 2.6284\n",
      "Epoch 9/50 | Loss: 2.5275\n",
      "Epoch 10/50 | Loss: 2.4403\n",
      "Epoch 11/50 | Loss: 2.3610\n",
      "Epoch 12/50 | Loss: 2.2988\n",
      "Epoch 13/50 | Loss: 2.2319\n",
      "Epoch 14/50 | Loss: 2.1808\n",
      "Epoch 15/50 | Loss: 2.1248\n",
      "Epoch 16/50 | Loss: 2.0874\n",
      "Epoch 17/50 | Loss: 2.0344\n",
      "Epoch 18/50 | Loss: 1.9991\n",
      "Epoch 19/50 | Loss: 1.9569\n",
      "Epoch 20/50 | Loss: 1.9159\n",
      "Epoch 21/50 | Loss: 1.8512\n",
      "Epoch 22/50 | Loss: 1.8225\n",
      "Epoch 23/50 | Loss: 1.7990\n",
      "Epoch 24/50 | Loss: 1.7927\n",
      "Epoch 25/50 | Loss: 1.7659\n",
      "Epoch 26/50 | Loss: 1.7510\n",
      "Epoch 27/50 | Loss: 1.7350\n",
      "Epoch 28/50 | Loss: 1.7215\n",
      "Epoch 29/50 | Loss: 1.7087\n",
      "Epoch 30/50 | Loss: 1.6882\n",
      "Epoch 31/50 | Loss: 1.6779\n",
      "Epoch 32/50 | Loss: 1.6605\n",
      "Epoch 33/50 | Loss: 1.6387\n",
      "Epoch 34/50 | Loss: 1.6325\n",
      "Epoch 35/50 | Loss: 1.6205\n",
      "Epoch 36/50 | Loss: 1.6043\n",
      "Epoch 37/50 | Loss: 1.5918\n",
      "Epoch 38/50 | Loss: 1.5854\n",
      "Epoch 39/50 | Loss: 1.5739\n",
      "Epoch 40/50 | Loss: 1.5572\n",
      "Epoch 41/50 | Loss: 1.5274\n",
      "Epoch 42/50 | Loss: 1.5105\n",
      "Epoch 43/50 | Loss: 1.5003\n",
      "Epoch 44/50 | Loss: 1.4993\n",
      "Epoch 45/50 | Loss: 1.4884\n",
      "Epoch 46/50 | Loss: 1.4817\n",
      "Epoch 47/50 | Loss: 1.4797\n",
      "Epoch 48/50 | Loss: 1.4724\n",
      "Epoch 49/50 | Loss: 1.4615\n",
      "Epoch 50/50 | Loss: 1.4610\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9778e023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 62.04%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "acc = 100 * correct/total\n",
    "print(f\"Test accuracy: {acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6caf3612",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './vgg_model100.pth'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8130838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plant_disease",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
